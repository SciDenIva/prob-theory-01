## Независимость случайных величин

Пусть $\xi, \eta$ - случайные величины. Они независимы, если $\forall x,y\in \mathbb{R}$

$$
P(\xi \le x, \eta \le y) = P(\xi \le x)\cdot P(\eta \le y) = F_\xi(x)\cdot F_\eta(y)
$$

$F_\xi(x), F_\eta(y)$ - функции распределения.

Если  $\xi_1, \ldots, \xi_n$ - независимы в совокупности (взаимно независимы), $\forall k \le n$, $\forall i_1, \ldots, i_k$, все $i_k$ - попарно различны, $\forall x_{i_1}, \ldots x_{i_k}\in \mathbb{R}$

$$
P(\xi_{i_1} \le x_{i_1}, \ldots, \xi_{i_k} \le x_{i_k}) = P(\xi_{i_1} \le x_{i_1})\times \dots \times P(\xi_{i_k} \le x_{i_k})
$$

Из попарной независимости не следует совокупная независимость.

Пусть $A,B,C$ - попарно независимые события, но зависимые в совокупности.

С каждым событием связана случайная величина $I_A, I_B, I_C$, которые называют _индикаторами_.

$$
I_A(w) =
\Bigg\{
\begin{array}{rl}
    1, & w \in A  \\
    0, & w \not\in A
\end{array}
$$

Если случайные величины (индикаторы) дискретны, то для независимости достаточно проверить, что
$$
P(\xi_{i_1} = x_{i_1}, \ldots, \xi_{i_k} = x_{i_k}) = P(\xi_{i_1} = x_{i_1})\times \dots \times P(\xi_{i_k} = x_{i_k})
$$

Пусть $\xi, \eta$ - случайные величины.

_Кавариация_ - совместная вариация, обозначается $cov(\xi, \eta) = E\Big((\xi - E\xi)(\eta - E\eta)\Big)$

$D$ - называют _вариацией_. $cov (\xi, \xi) = D\xi$.

_Коэффициент корреляции_ $\displaystyle\rho(\xi, \eta) = \frac{cov(\xi,\eta)}{\sqrt{D\xi \cdot D\eta}}$. При этом всегда $-1 \le \rho(\xi,\eta) \le 1$.

$\rho(\xi,\eta)$ похожа на $cos$ угла между случайными величинами. Если $\rho(\xi,\eta) = 0$, то случайные величины "ортогональны" и это говорит об их "несвязанности" между собой.

Случайные величины $\xi, \eta$, у которых $cov(\xi,\eta)=0$ называют _некоррелированными_.

**Утверждение**

Если $\xi,\eta$ - независимы, то $E(\xi\eta) = E(\xi)\cdot E(\eta)$

$\square$

$\xi:\Omega \rightarrow \{ y_1, \ldots, y_k\}$

$\eta:\Omega \rightarrow \{ z_1, \ldots, z_m\}$


$$
E(\xi\eta) = \sum\limits_{i=1}^k\sum\limits_{j=1}^m y_i z_j P(\xi=y_i, \eta=z_j) = \sum\limits_{i=1}^k\sum\limits_{j=1}^m y_i z_j P(\xi=y_i) P(\eta=z_j) = \\
\ \\
= \Bigg(\sum\limits_{i=1}^k y_i P(\xi=y_i) \Bigg) \Bigg(\sum\limits_{j=1}^m z_i P(\eta=z_j) \Bigg)= (E\xi)(E\eta)\\
$$

$\blacksquare$


Имеем

$cov(\xi, \eta) = E\Big((\xi - E\xi)(\eta - E\eta)\Big) = E(\xi\eta) - E\xi E\eta = 0$ для некоррелированных случайных величин $\Rightarrow E(\xi\eta) = E\xi E\eta$

Т.е. если случайные величины независимы, то они также и некоррелированны.

Обратное не верно.

**Пример**

$\xi \thicksim R(0, 2\pi)$, $\eta_1 = \sin \xi$, $\eta_2 = \cos \xi$

$\eta_1^2 + \eta_2^2 = 1$, т.е. эти случайные величины - зависимы.

$$
E(\eta_1\eta_2) = E\Big( (\sin\eta) (\cos\eta)\Big) = E\Big( \frac{\sin 2\xi}{2}\Big) = \frac{1}{2}\int\limits_0^{2\pi} \frac{1}{2\pi} \sin 2x dx = 0
$$

$cov(\eta_1\eta_2) = 0$, т.е. они не коррелированны, но зависимы.

**Теорема**

Если $\xi_1, \ldots, \xi_n$ независимы в совокупности, то $E(\xi_1, \ldots, \xi_n) = (E\xi_1) \times \cdots \times (E\xi_n)$.

**Теорема**

Если $\xi,\eta$ - некоррелированы, то $D(\xi + \eta) = D\xi + D\eta$.

$\square$

$$
D(\xi + \eta) = E (\xi + \eta)^2 - (E(\xi + \eta))^2 = \\
\ \\
= E\xi^2 + 2E(\xi\eta) + E\eta^2 - (E\xi)^2 - 2 E\xi E\eta - (E\eta)^2 = \\
\ \\
= E\xi^2 - (E\xi)^2 + E\eta^2  - (E\eta)^2 = D\xi + D\eta
$$

$\blacksquare$

**Теорема**

Если $\xi_1, \ldots, \xi_n$ попарно некоррелированны, то $D(\xi_1 + \cdots + \xi_n) = D\xi_1 + \cdots + D\xi_n$.

**Пример** Случайный граф $G(n,p)$

$X(G)$ - случайная величина, которая как и любая случайная величина зависит от элементарного исхода. Элементарным исходом является граф.

Случайная величина, как функция на множестве графов $X(G)$ принимает значение - количество треугольников в графу $G$, т.е. количество полных подграфов на трех вершинах. Найти вероятность $P(X = 0)$

## Математическое ожидание случайной величины

Рассмотрим случайную величину с дискретным распределением.

$X: \Omega=\{w_1,\ldots, w_n\}$, пусть $p_1,\ldots, p_n$ - вероятность этих элементарных исходов.

Обозначения для математического ожидания: $MX$, $EX$.

$$
EX=\sum_{i=1}^n X(w_i)\cdot p_i
$$

$EX$ - центр масс.

Пусть случайная величина $X$ на множестве различных исходов $\{w_1,\ldots, w_n\}$ принимает возможные значения $y_1,\ldots, y_k$, где $1\le k \le n$. Может так случиться, что $X(w_i) = X(w_j) = y_m$.

Переформатируем слагаемые в $EX$.

$$
EX = y_1\Big( \sum_{i:X(w_i)=y_1} p_i\Big) + y_2\Big( \sum_{i:X(w_i)=y_2} p_i\Big) + \cdots + y_k\Big( \sum_{i:X(w_i)=y_k} p_i\Big) = \\
\ \\
= y_1\cdot P(x = y_1) + y_2\cdot P(x = y_2) + \cdots + y_k\cdot P(x = y_k) = \sum_{i=1}^k y_i\cdot P(X = y_i)
$$

$P(X=y_1)$ - вероятность события, состоящего из $w_i:X(w_i)=y_1$. $w_i$ - элементарные события и они не пересекаются.

Если дискретная случайная величина принимает счетное множество значений $X:\Omega\rightarrow\{y_n\}_{n=1}^{\infty}$.

$$
EX=\sum_{i=1}^\infty y_n\cdot P(X=y_n)
$$

Абсолютно непрерывная случайная величина $p(y)\ge 0$, $\displaystyle\int\limits_{-\infty}^\infty p(y) dy = 1$

$$
EX = \int\limits_{-\infty}^\infty y\cdot p(y) dy
$$

Если сравнивать с дискретными случайными величинами, то аналогом $P(X = y_n)$ является $p(y)dy$.

> Теорема Лебега. Помимо дискретного и абсолютно непрерывного распределения есть сингулярное распределение и все существующие распределения могут быть представлены как комбинация из этих трех вариантов с некоторыми коэффициентами.

## Линейность математического ожидания

$$
E(c_1X_1 + c_2X_2) = c_1 EX_1 + c_2 EX_2
$$

Математическое ожидание от случайной величины - это некоторое число. $E$ - это функция от функции. Случайная величина - это функция, а мы к ней применяем специальную операцию, которая превращает эту функцию в число.

В математике такие операции называют функционалом. $E$ - это функционал.

> Нельзя писать $EX(w)$. Т.е. $X$ - зависит от $w$ - элементарного исхода, но $E$ ни от какого исхода не зависит.

**Пример**

$X(G)$ - число треугольников. $EX = \sum\limits_{k=0}^{C_n^3} k \cdot P(X = k)$.

Если сравнить эту формулу с $EX = \sum\limits_{k=0}^n y_k \cdot P(X = k)$, то $y_k = k$.

$y_k$ - это значение случайной величины. В случае с треугольниками, $k$ - это количество треугольников и это и есть значение случайной величины $X$.

Представим $X(G)$ через линейную комбинацию других случайных величин.

$$
X(G) = X_1(G) + \cdots X_{C_n^3}(G)
$$

Введем $t_1, t_2, \ldots, t_{C_n^3}$ - это номера всех возможных троек вершин.

$$
X_i(G) =
\Bigg\{
\begin{array}{rl}
1, & t_i\ образует\ треугольник\ в\ G \\
0, & иначе \\
\end{array}
$$

$$
EX = EX_1 + \cdots + EX_{C_n^3}
$$

Чему равно $EX_i$ ?

$$
EX_i = 1\cdot P(X_i = 1) + 0 \cdot P(X_i = 0) = P(X_i = 1) = p^3
$$

$P(X_i) = 1$ - вероятность, что конкретные $3$ числа являются вершинами графа, между которыми попарно есть ребра. Но каждое ребро возникает в графе независимо от другого с вероятностью $p$.

$$
EX = C_n^3\cdot p^3
$$

С одной стороны, математическое ожидание определяется через вероятности, но за счет линейности оно может быть подсчитано показанным образом при том, что не известна ни одна из вероятностей.

**Пример**

$X \thicksim Binom(n,p)$, $n$ раз бросили монету и $p$ - вероятность успеха.

$$
X = X_1 + \cdots + X_n,\ где\ X_i=
\Bigg\{
\begin{array}{rl}
    1, & успех\\
    0, & неудача\\
\end{array}
$$

$$
EX = EX_1 + \cdots + EX_n = np, так\ как\ EX_i = 1\cdot p + 0 \cdot (1-p) = p
$$

Математическое ожидание можно подсчитать через вероятность.

$$
EX = \sum\limits_{k=0}^n k \cdot C_n^k p^k q^{n-k} = \sum\limits_{k=1}^n k \cdot C_n^k p^k q^{n-k} = \\
\ \\
= \sum\limits_{k=1}^n k \cdot \frac{n!}{k!(n-k)!} p^k q^{n-k} = \sum\limits_{k=1}^n \frac{n(n-1)!}{(k-1)!(n-k)!} p \cdot p^{k-1} q^{n-k} = \\
\ \\
= np \sum\limits_{k=1}^n \frac{(n-1)!}{(k-1)!(n-k)!} p^{k-1} q^{n-k} = np \sum\limits_{k=1}^n C_{n-1}^{k-1} p^{k-1} q^{(n-1)-(k-1)} = \\
\ \\
= np (p + q)^{n-1} = np
$$

**Пример**

$$
X \thicksim R(a,b), EX = \frac{a+b}{2}
$$

Пусть $X$ - случайная величина и к ней применяется функция $f$, т.е. получаем новую случайную величину $f(X)$. Как найти $Ef(X)?$

Например, рассмотрим $EX^2$.

$$
P(X^2 = y_i) = P(X \in \{\sqrt{y_i}, -\sqrt{y_i}\})
$$

Тогда нужно считать

$$
EX^2 = \sum_i y_i\cdot P(X^2 = y_i)
$$

Интуитивно понятно, что для $X:\Omega\rightarrow\{y_1, \ldots, y_n, \ldots\}$

$$
Ef(X) = \sum_{i=1}^{\infty} f(y_i)\cdot P(X=y_i)
$$

$$
Ef(X) = \int_{-\infty}^{\infty} f(y)\cdot p(y) dy
$$

## Моменты распределения

Математическое ожидание - это $1$-ый момент.

$EX^k$ - $k$-ый момент случайной величины $X$.

$E_f^kX$ - $k$-ый факториальный момент случайной величины $X$.

$$
E_f^kX = E\Big( X \cdot (X-1) \times \cdots \times (X-k+1) \Big)
$$

$DX$ - дисперсия случайной величины, один из вариантов $2$-ого момента - среднеквадратичное отклонение

$$
DX = E\Big( (X - EX)^2 \Big) = E\Big( X^2 - 2(EX)X + (EX)^2 \Big) = \\
\ \\
= EX^2 - 2(EX)(EX)+(EX)^2 = EX^2 - (EX)^2
$$

$E(2(EX)) = 2(EX)$, т.к. $2(EX)$ - это константа.

$E((EX)^2) = (EX)^2$, т.к. $(EX)^2$ - это константа.

$DX$ - не обладает свойством линейности.

$$
D(cX) = c^2DX
$$

$D(X_1 + X_2) = DX_1 + DX_2$ - в некоторых случаях

**Пример**

$X \thicksim Poisson(\lambda)$, $\lambda > 0$, $\displaystyle P(X=i) = \frac{\lambda^i \cdot e^{-\lambda}}{i!}, i\in{0,1,\ldots}$

$$
E_f^kX = \sum\limits_{i=0}^\infty i(i-1)\times \cdots \times (i-k+1)\frac{\lambda^i\cdot e^{-\lambda}}{i!} = \\
\ \\
= \sum\limits_{i=k}^\infty i(i-1)\times \cdots \times (i-k+1)\frac{\lambda^i\cdot e^{-\lambda}}{i!} = \sum\limits_{i=k}^\infty \frac{\lambda^i\cdot e^{-\lambda}}{(i-k)!} = \\
\ \\
= \lambda^k e^{-\lambda} \sum\limits_{i=k}^\infty \frac{\lambda^{i-k}\cdot }{(i-k)!} = \lambda^k e^{-\lambda} \sum\limits_{j=0}^\infty \frac{\lambda^j\cdot }{j!} = \lambda^k e^{-\lambda} e^{\lambda} = \lambda^k
$$

По предельной теореме Пуассона в схеме испытаний Бернулли

$$
p \thicksim \frac{c}{n} \Rightarrow C_n^kp^kq^{n-k} \rightarrow \frac{c^k e^{-c}}{k!}
$$

$\displaystyle p\thicksim \frac{c}{n}$ то же, что и $c \thicksim pn$.

$pn$ - математическое ожидание биномиальной случайной величины.

$c$ - это $\lambda$, т.е. математическое ожидание пуассоновской случайной величины.

$\lambda$ в пуассоновском распределении - это математическое ожидание.